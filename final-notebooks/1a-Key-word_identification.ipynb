{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word identification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we identify key words in sentences that are likely to correspond to the Wikipedia article(s) that are most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from string import punctuation, digits\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question data\n",
    "t_data = pd.read_pickle('C:/Users/Fredi/Kodningsprojekt/data-analysis/workproduct-files/cleaned-dataframes/t_dataMaster-duplicatesRemoved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency data\n",
    "word_frequencies = pd.read_csv(\"F:/Word frequencies/unigram_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namedEntities(a):\n",
    "    doc = nlp(a)\n",
    "    toReturn = []\n",
    "    \n",
    "    #If input is title-case, convert to sentence\n",
    "    if a.istitle():\n",
    "        a = a.capitalize()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        toReturn.append((ent.text, ent.label_))\n",
    "\n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNouns(a):\n",
    "    \n",
    "    #Identify substrings between quotes\n",
    "    \n",
    "    quotes1 = re.compile('\"[^\"]*\"')\n",
    "    quotes2 = re.compile(\"'[^']*'\")\n",
    "    quoteRanges = []\n",
    "    toReturn = []\n",
    "    \n",
    "    for value in quotes1.finditer(a.replace(\"'s\", \"!s\")):\n",
    "        quoteRanges.append(value.span())\n",
    "        toReturn.append(value.group().replace(\"!s\", \"'s\")[1:-1])\n",
    "    for value in quotes2.finditer(a.replace(\"'s\", \"!s\")):\n",
    "        quoteRanges.append(value.span())\n",
    "        toReturn.append(value.group().replace(\"!s\", \"'s\")[1:-1])\n",
    "    \n",
    "    #If input is title-case, convert to sentence\n",
    "    if a.istitle():\n",
    "        a = a.capitalize()\n",
    "    \n",
    "    doc = nlp(a)\n",
    "    pos_tag = ['NOUN']\n",
    "    \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        \n",
    "        #Exclude words within quotes\n",
    "        if any(start <= token.idx <= end for (start, end) in quoteRanges):\n",
    "            continue\n",
    "        \n",
    "        if(token.pos_ in pos_tag):\n",
    "            toReturn.append(token.lemma_)\n",
    "    \n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjects(a):\n",
    "    doc = nlp(a.lower())\n",
    "    toReturn = []\n",
    "    dep_tag = ['csubj', 'nsubj']\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.dep_ in dep_tag):\n",
    "            toReturn.append(token.lemma_)\n",
    "    \n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjects(a):\n",
    "    doc = nlp(a.lower())\n",
    "    toReturn = []\n",
    "    dep_tag = ['iobj', 'obj', 'dobj', 'pobj']\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.dep_ in dep_tag):\n",
    "            toReturn.append(token.lemma_)\n",
    "    \n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencyCount(a):\n",
    "    result = []\n",
    "    for word in a:\n",
    "        try:\n",
    "            result.append([word, word_frequencies.loc[word_frequencies[\"word\"] == word, \"count\"].iloc[0] ])\n",
    "        except IndexError:\n",
    "            # If no count is found, add 0 as value to ensure being sorted first in searchTerms()\n",
    "            result.append([word, 0])\n",
    "    return result    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Identify wikipedia redirect terms to look for (in priority order)\n",
    "# 1. Check if named entities exist and add in order of priority (WORK_OF_ART, PERSON, FAC, LOC)\n",
    "# 2. Add nouns in ascending frequency order (rarest first)\n",
    "# 3. Remove \"the\" / \"a\" / \"an\" / quotes from beginning of named entities\n",
    "\n",
    "# Takes a question row as input\n",
    "\n",
    "\n",
    "def searchTerms(a):\n",
    "    result = []\n",
    "    \n",
    "    for ent in a[\"namedEntities\"]:\n",
    "        if ent[1] == \"WORK_OF_ART\":\n",
    "            result.append(ent[0])\n",
    "    for ent in a[\"namedEntities\"]:\n",
    "        if ent[1] == \"PERSON\":\n",
    "            result.append(ent[0])    \n",
    "    for ent in a[\"namedEntities\"]:\n",
    "        if ent[1] == \"FAC\" or ent[1] == \"LOC\":\n",
    "            result.append(ent[0])\n",
    "    \n",
    "    # Update to account for objects and subjects\n",
    "    \n",
    "    # sort nounsObjectsSubjects by nounFrequencies\n",
    "    # https://www.kite.com/python/answers/how-to-sort-a-list-of-lists-by-an-index-of-each-inner-list-in-python\n",
    "    # sorted_list = sorted(nested_list, key=lambda x: x[1])\n",
    "    wordsSorted = sorted(a[\"nounsObjectsSubjects\"], key=lambda x: x[1])\n",
    "    for word in wordsSorted:\n",
    "        result.append(word[0])\n",
    "        \n",
    "    # Clean-up, iterate over each value in result list          \n",
    "    for i in range(len(result)):\n",
    "        try:\n",
    "            if result[i][:4].lower() == \"the \":\n",
    "                result[i] = result[i][4:]\n",
    "            if result[i][:2].lower() == \"a \":\n",
    "                result[i] = result[i][2:]\n",
    "            if result[i][:3].lower() == \"an \":\n",
    "                result[i] = result[i][3:]\n",
    "            if result[i][0].lower() == \"'\" or result[i][0].lower() == '\"':\n",
    "                result[i] = result[i][1:]\n",
    "            if result[i][-1].lower() == \"'\" or result[i][-1].lower() == '\"':\n",
    "                result[i] = result[i][:-1]\n",
    "            if result[i][-2:].lower() == \"'s\":\n",
    "                result[i] = result[i][:-2]\n",
    "        except:\n",
    "            continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions[\"namedEntities\"] = questions[\"CONS_question\"].apply(lambda x: namedEntities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract namedEntities from answer string\n",
    "questions[\"namedEntities_answer\"] = questions[\"CONS_answer\"].apply(lambda x: namedEntities(x))\n",
    "\n",
    "# Combine namedEntities from question and answer into same list\n",
    "questions[\"namedEntities\"] = questions[\"namedEntities\"] + questions[\"namedEntities_answer\"]\n",
    "\n",
    "# Drop namedEntities_answer column\n",
    "questions = questions.drop(\"namedEntities_answer\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions[\"nouns\"] = questions[\"CONS_question\"].apply(lambda x: getNouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions[\"objects\"] = questions[\"CONS_question\"].apply(lambda x: getObjects(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions[\"subjects\"] = questions[\"CONS_question\"].apply(lambda x: getSubjects(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine nouns, objects and subjects into same list\n",
    "questions[\"nounsObjectsSubjects\"] = questions[\"nouns\"] + questions[\"objects\"] + questions[\"subjects\"]\n",
    "# Remove duplicates\n",
    "questions[\"nounsObjectsSubjects\"] = questions[\"nounsObjectsSubjects\"].apply(lambda x: list(dict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add frequency count to nounsObjectsSubjects\n",
    "questions[\"nounsObjectsSubjects\"] = questions[\"nounsObjectsSubjects\"].apply(lambda x: frequencyCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions[\"searchTerms\"] = questions.apply(searchTerms, axis = 1)\n",
    "# Remove duplicates\n",
    "questions[\"searchTerms\"] = questions[\"searchTerms\"].apply(lambda x: list(dict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.to_pickle(\"../workproduct-files/t_dataMaster-keywordsIdentified.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
