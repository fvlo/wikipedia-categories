{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# cypher.forbid_exhaustive_shortestpath=true set in neo4j conf file\n",
    "# https://neo4j.com/docs/operations-manual/current/configuration/neo4j-conf/\n",
    "\n",
    "# dbms.transaction.timeout and dbms.lock.acquisition.timeout set to 10s in neo4j.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation, digits\n",
    "import re\n",
    "import ast\n",
    "import stopit\n",
    "from py2neo import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n"
     ]
    }
   ],
   "source": [
    "print(\"Started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword to article title mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "redirects = pd.read_csv(\"F:/wikipedia-data/outputs/redirect.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "articles = pd.read_csv(\"F:/wikipedia-data/outputs/articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "articles.dropna(subset = [\"title\"], inplace = True)\n",
    "redirects.dropna(subset = [\"title\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "articles[\"titleLower\"] = articles[\"title\"].apply(lambda x: x.lower())\n",
    "redirects[\"titleLower\"] = redirects[\"title\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to neo4j database - start database separately\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeout-limit in seconds for database calls\n",
    "maxSearchTime = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trivia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "t_data = pd.read_pickle(\"../workproduct-files/t_dataMaster-keywordsIdentified.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search term to wikipedia article name linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns wikipedia article formatted for database search, if not found, returns FALSE\n",
    "def inArticles(a):\n",
    "    match = articles.loc[articles[\"titleLower\"] == a.lower(), :]\n",
    "    if len(match) > 0:\n",
    "        return match.iloc[0, 1].replace(\" \", \"_\")\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns wikipedia article formatted for database search, if not found, returns FALSE\n",
    "def inRedirects(a):\n",
    "    match = redirects.loc[redirects[\"titleLower\"] == a.lower(), :]\n",
    "    if len(match) > 0:\n",
    "        return match.iloc[0, 2].replace(\" \", \"_\")\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get first link from article based on title (DB formatting). Return False if no links exist\n",
    "def getFirstLink(a):\n",
    "    #match will be a pandas series of len=1\n",
    "    match = articles.loc[articles[\"title\"] == a.replace(\"_\", \" \"), \"links\"]\n",
    "    \n",
    "    if len(match) > 0:\n",
    "        #Change first series value into list\n",
    "        asList = ast.literal_eval(match.iloc[0]) \n",
    "        #result = asList[0].replace(\" \", \"_\")\n",
    "        result = asList[0]\n",
    "        \n",
    "        #Take string only until |\n",
    "        result = re.sub(\"(\\|)(.+)\", '', result)\n",
    "        result = re.sub(\"(\\|)\", '', result)\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia article name to neo4j database calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call database for category tree and parents of given wikipedia title\n",
    "@stopit.threading_timeoutable(default='Database call timed out (' + str(maxSearchTime) + ' seconds)')\n",
    "def getCategoryInfo(a):\n",
    "    # kill function if runs to long (>2min ?)\n",
    "        # https://stackoverflow.com/questions/14920384/stop-code-after-time-period\n",
    "        # https://pypi.org/project/stopit/#id14\n",
    "    \n",
    "    #result [wikipediID, path to MTC, parents]\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        articleID = articleByTitle(a).iloc[0,1]\n",
    "        parents = parentCategories(articleID)\n",
    "        \n",
    "        if \"Disambiguation_pages\" in parents[\"pages.title\"].values:\n",
    "            firstLink = getFirstLink(a)\n",
    "            return getWikipediaInfo(firstLink)\n",
    "        else:\n",
    "            path = chosenPathArticleToMTC(articleID)\n",
    "        \n",
    "        return [a, articleID, path, parents]\n",
    "    \n",
    "    except (IndexError, ValueError, TypeError, ClientError, AttributeError):\n",
    "        return \"Database call not successful (error)\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs search functions from given search term --> Output from wikipedia database\n",
    "def getWikipediaInfo(a):\n",
    "    \n",
    "    term = a.lower()\n",
    "    \n",
    "    out = inArticles(term)\n",
    "    if out != False:\n",
    "        return getCategoryInfo(out, timeout = maxSearchTime)\n",
    "        #return getCategoryInfo(out)\n",
    "    \n",
    "    out = inRedirects(term)\n",
    "    if out != False:\n",
    "        return getCategoryInfo(out, timeout = maxSearchTime)\n",
    "        #return getCategoryInfo(out)\n",
    "    \n",
    "    return \"Search term not found\"\n",
    "        \n",
    "    \n",
    "    # if search term is in articles\n",
    "        # Perform database search\n",
    "        # Return (WikipediaID, Category tree, Parent categories)\n",
    "    # else if search term is in redirects\n",
    "        # Perform database search\n",
    "        # Return (WikipediaID, Category tree, Parent categories)\n",
    "    # else return FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neo4j database calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return node info based on wikipedia id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodeInfo(a):\n",
    "    commandToRun = 'MATCH (pages:Page {id: %s}) \\\n",
    "                RETURN pages' % (a)\n",
    "    return graph.run(commandToRun).data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return similarity statistics for two sets (intersection, union, Jaccard coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity statistics\n",
    "def similarityStats(a,b):\n",
    "    intSize = len(a.intersection(b))\n",
    "    unionSize = len(a.union(b))\n",
    "    \n",
    "    if unionSize == 0:\n",
    "        jaccard = 0\n",
    "    else:\n",
    "        jaccard = intSize / unionSize\n",
    "    \n",
    "    return (intSize, unionSize, jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return identifying information of parent categories of chosen article or category as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give wikipedia id as integer as function argument (e.g. cateogry \"Finland\" = 693995)\n",
    "def parentCategories(a):\n",
    "    wikiID = a\n",
    "    commandToRun = 'MATCH (pages:Category:Page) \\\n",
    "                <-[:BELONGS_TO]- \\\n",
    "                (:Page {id: %s}) \\\n",
    "                RETURN pages.title, pages.id' % (wikiID)\n",
    "\n",
    "    # ensure that a dataframe with correct columns is returnd also when query is empty\n",
    "    out = pd.DataFrame(columns = [\"pages.title\", \"pages.id\"])\n",
    "    out = out.append(graph.run(commandToRun).to_data_frame())\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return identifying information of children (both category and article) of chosen category as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give wikipedia id as integer as function argument (e.g. cateogry \"Finland\" = 693995)\n",
    "def childPages(a):\n",
    "    wikiID = a\n",
    "    commandToRun = 'MATCH (pages:Page) \\\n",
    "                -[:BELONGS_TO]-> \\\n",
    "                (:Page {id: %s}) \\\n",
    "                RETURN pages.title, pages.id' % (wikiID)\n",
    "\n",
    "    # ensure that a dataframe with correct columns is returnd also when query is empty\n",
    "    out = pd.DataFrame(columns = [\"pages.title\", \"pages.id\"])\n",
    "    out = out.append(graph.run(commandToRun).to_data_frame())\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe with all articles (but not categories) with given title [only one result is expected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give wikipedia title as string as function argument\n",
    "# Will not work if article title contains \"-character --> \"ClientError\". Escape fixes do not work, not worth debugging.\n",
    "def articleByTitle(a):\n",
    "    ArticleToFind = a\n",
    "    commandToRun = 'MATCH (articles:Page {title: \"%s\"}) \\\n",
    "                    WHERE NONE(art IN [articles] WHERE art:Category) \\\n",
    "                    RETURN articles.title, articles.id, ID(articles)' % (ArticleToFind)\n",
    "    return graph.run(commandToRun).to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe with all categories (but not articles) with given title [only one result is expected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give wikipedia title as string as function argument\n",
    "def categoryByTitle(a):\n",
    "    CategoryToFind = a\n",
    "    commandToRun = 'MATCH (categories:Category:Page {title: \"%s\"}) \\\n",
    "                    RETURN categories.title, categories.id, ID(categories)' % (CategoryToFind)\n",
    "    return graph.run(commandToRun).to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe containing shortest path between input node (article or category) and Main_topics_classification category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestPathToMTC(a):\n",
    "    # (:Page {id: 7345184}) is Main_topics_classifications category node\n",
    "    inputNode = a\n",
    "\n",
    "    commandToRun = 'MATCH path=shortestPath( \\\n",
    "                    (:Page {id: %s})-[:BELONGS_TO*0..10]->(:Page {id: 7345184})) \\\n",
    "                    UNWIND nodes(path) AS pages \\\n",
    "                    RETURN pages.title, pages.id, ID(pages)' % (inputNode)\n",
    "\n",
    "    return pd.DataFrame(graph.run(commandToRun).data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return similarity value between two categories as defined by Biuk-Aghai & Cheang (2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take as input tuple containing depth of category to compare to as well as intersection of children between two categories\n",
    "\n",
    "'''\n",
    "Given parent category p and child category c,\n",
    "and given a root category node r, we calculate the category similarity\n",
    "Sp;c as: Sp;c = Dc - Cp;c / k , where Dc is the depth of category\n",
    "c in the category graph, i.e. the shortest distance from the root\n",
    "category node r; Cp;c is the number of co-assigned articles of categories\n",
    "p and c; and k is a constant that is empirically determined.\n",
    "Through experimentation we have found that a value of k = 2 produces\n",
    "the best results, i.e. results that agree with human intuition as\n",
    "to similarity of a given pair of categories. A smaller value of Sp;c\n",
    "indicates a greater similarity (i.e. a smaller distance between the\n",
    "nodes). The number of co-assigned articles Cp;c of parent category\n",
    "p and child category c is simply the cardinality of the intersection\n",
    "of their assigned article sets: Cp;c = jAp \\ Acj, where Ap and Ac\n",
    "are the sets of articles assigned to categories p and c, respectively.\n",
    "'''\n",
    "# Depth is calcualted for parent when going bottom-up in graph\n",
    "# C is calculated using intersection of both child articles and sub-categories\n",
    "\n",
    "def similarityBAC(a):\n",
    "    d = a[0]\n",
    "    c = a[1]\n",
    "    k = 2\n",
    "    return d - (c/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe containing all parent categories of category a and similarity statistics to each as well as parent depth to MTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give wikipedia id as integer as function argument (e.g. cateogry \"Finland\" = 693995)\n",
    "def parentSimilarities(a):\n",
    "    parents = parentCategories(a)\n",
    "    children = childPages(a)\n",
    "    \n",
    "    if len(parents) == 0:\n",
    "        raise ValueError(\"Category processed does not have parents (likely input category to chosenPathUpToMTC() if called)\")\n",
    "    \n",
    "    # Create columns with similarity stats using functions similarityStats\n",
    "    parents[\"similarities\"] = parents[\"pages.id\"].apply(lambda x: similarityStats(set(children[\"pages.id\"]), set(childPages(x)[\"pages.id\"])))\n",
    "    parents[[\"intersection\", \"union\", \"jaccard\"]] = pd.DataFrame(parents['similarities'].tolist(), index = parents.index)\n",
    "    parents.drop([\"similarities\"], axis = 1, inplace = True)\n",
    "        \n",
    "    # Add column with parent category depth (steps to Main_topics_classifications node)\n",
    "    parents[\"depth\"] = parents[\"pages.id\"].apply(lambda x: len(shortestPathToMTC(x))-1)\n",
    "    \n",
    "    # Add column with similarityBAC\n",
    "    parents[\"similarityBAC-aid\"] = list(zip(parents[\"depth\"], parents[\"intersection\"]))\n",
    "    parents[\"similarityBAC\"] = parents[\"similarityBAC-aid\"].apply(lambda x: similarityBAC(x))\n",
    "    parents.drop([\"similarityBAC-aid\"], axis = 1, inplace = True)\n",
    "    \n",
    "    # Sort ascending\n",
    "    parents.sort_values(by = \"similarityBAC\", ascending = True, inplace = True)\n",
    "    parents.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return node based on neo4j database ID [NOTE: not same as wikipedia ID used elsewhere]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWithNeoID(a):\n",
    "    return NodeMatcher(graph).get(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe containing info of what category to choose from parentSimilarities() output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we choose which parent link to keep according to\n",
    "following rules: (1) Choose the parent whose similarity value Sp;c\n",
    "is lower; (2) If Sp1;c = Sp2;c, choose the parent whose depth D is\n",
    "lower; (3) If Dp1 = Dp2, choose the parent with the larger value\n",
    "of Cp;c; (4) If Cp1;c = Cp2;c, choose the parent with the lower\n",
    "page ID.\n",
    "'''\n",
    "# Takes parentSimilarities() / or potentially child similarities output dataframe as input\n",
    "def chooseCategoryPath(a):\n",
    "    a.sort_values(by = [\"similarityBAC\", \"depth\"], ascending = True, inplace = True)\n",
    "    a[\"mostSimilar\"] = \"False\"\n",
    "    a[\"comment\"] = \"\"\n",
    "    \n",
    "    # Set value for mostSimilar to \"Not connected\" for rows with depth = -1 i.e. no connection to MTC\n",
    "    a.loc[a[\"depth\"] == -1, \"comment\"] = \"Not connected\"\n",
    "    \n",
    "    # Set value for mostSimilar to \"True\" for rows that are not \"Not connected\" and that have the minimum value of similarityBAC\n",
    "    workingDF = a.loc[a[\"comment\"] != \"Not connected\"]\n",
    "    selectedIndexes = workingDF.loc[workingDF[\"similarityBAC\"] == workingDF[\"similarityBAC\"].min()].index\n",
    "    \n",
    "    a.loc[selectedIndexes, \"mostSimilar\"] = \"True\"\n",
    "    a.loc[selectedIndexes, \"comment\"] = \"Lowest similarityBAC\"\n",
    "    \n",
    "    workingDF = a.loc[a[\"mostSimilar\"] == \"True\"]\n",
    "    \n",
    "    if len(workingDF) > 1:\n",
    "        # Set all mostSimilar of partial dataframe and output back to False, then set min depth rows to true\n",
    "        workingDF[\"mostSimilar\"] = \"False\"\n",
    "        a[\"mostSimilar\"] = \"False\"\n",
    "        selectedIndexes = workingDF.loc[workingDF[\"depth\"] == workingDF[\"depth\"].min()].index\n",
    "        \n",
    "        a.loc[selectedIndexes, \"mostSimilar\"] = \"True\"\n",
    "        a.loc[selectedIndexes, \"comment\"] = a.loc[selectedIndexes, \"comment\"] + \"; Lowest depth\"\n",
    "        \n",
    "        workingDF = a.loc[a[\"mostSimilar\"] == \"True\"]\n",
    "        \n",
    "        # If several rows now set to true, test for highest intersection\n",
    "        if len(workingDF) > 1:\n",
    "            # Set all mostSimilar of partial dataframe and output back to False, then set max intersection rows to true\n",
    "            workingDF[\"mostSimilar\"] = \"False\"\n",
    "            a[\"mostSimilar\"] = \"False\"\n",
    "            selectedIndexes = workingDF.loc[workingDF[\"intersection\"] == workingDF[\"intersection\"].max()].index\n",
    "            \n",
    "            a.loc[selectedIndexes, \"mostSimilar\"] = \"True\"\n",
    "            a.loc[selectedIndexes, \"comment\"] = a.loc[selectedIndexes, \"comment\"] + \"; Highest intersection\"\n",
    "            \n",
    "            workingDF = a.loc[a[\"mostSimilar\"] == \"True\"]\n",
    "            \n",
    "            # If several rows now set to true, choose row with lowes pages.id\n",
    "            if len(workingDF) > 1:\n",
    "                # Set all mostSimilar of partial dataframe and output back to False, then set min wikipedia id row (only one) to true\n",
    "                workingDF[\"mostSimilar\"] = \"False\"\n",
    "                a[\"mostSimilar\"] = \"False\"\n",
    "                selectedIndexes = workingDF.loc[workingDF[\"pages.id\"] == workingDF[\"pages.id\"].min()].index\n",
    "                \n",
    "                a.loc[selectedIndexes, \"mostSimilar\"] = \"True\"\n",
    "                a.loc[selectedIndexes, \"comment\"] = a.loc[selectedIndexes, \"comment\"] + \"; Lowest wikipedia id\"\n",
    "    \n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return dataframe containing info of chosen path to MTC (iterates chooseCategoryPath() upwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate chooseCategoryPath() from input category (wikipedia id as input) until MTC is reached. Return dataframe with chosen path rows\n",
    "# Root node category \"Main_topic_classifications\" has pages.id = 7345184\n",
    "\n",
    "# NOTE: Error if input category does not have parents\n",
    "def chosenPathUpToMTC(a):\n",
    "    mtcFound = False\n",
    "    nextStep = a\n",
    "    chosenPath = pd.DataFrame()\n",
    "    \n",
    "    while(not mtcFound):\n",
    "        allParents = parentSimilarities(nextStep)\n",
    "        allParents = chooseCategoryPath(allParents)\n",
    "        \n",
    "        # If allParents contains MTC category\n",
    "        if(len(allParents.loc[allParents[\"pages.id\"] == 7345184]) == 1):\n",
    "            rowToAppend = allParents.loc[allParents[\"pages.id\"] == 7345184]\n",
    "            mtcFound = True\n",
    "        else:\n",
    "            rowToAppend = allParents.loc[allParents[\"mostSimilar\"] == \"True\"]\n",
    "            nextStep = int(allParents.loc[allParents[\"mostSimilar\"] == \"True\", \"pages.id\"])\n",
    "        \n",
    "        chosenPath = chosenPath.append(rowToAppend)\n",
    "        chosenPath.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    \n",
    "    return chosenPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Article strength calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dataframe with all pages linking to or from input page\n",
    "def linksBetween(a):\n",
    "    wikiID = a\n",
    "    commandToRun = 'MATCH (pages:Page) \\\n",
    "                -[:LINKS_TO]- \\\n",
    "                (:Page {id: %s}) \\\n",
    "                RETURN pages.title, pages.id' % (wikiID)\n",
    "\n",
    "    # ensure that a dataframe with correct columns is returnd also when query is empty\n",
    "    out = pd.DataFrame(columns = [\"pages.title\", \"pages.id\"])\n",
    "    out = out.append(graph.run(commandToRun).to_data_frame())\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a as pages.id for artice, c as pages.id for parent category\n",
    "def articleClassificationStrength(a, c):\n",
    "    aLinks = set(linksBetween(a)[\"pages.id\"])\n",
    "    cChildren = set(childPages(c)[\"pages.id\"])\n",
    "    \n",
    "    intersectionSize = len(aLinks.intersection(cChildren))\n",
    "    \n",
    "    return 1 + intersectionSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongestArticleParents(a):\n",
    "    parents = parentCategories(a)\n",
    "    parents[\"depth\"] = parents[\"pages.id\"].apply(lambda x: len(shortestPathToMTC(x)) -1 )\n",
    "    parents.loc[parents[\"depth\"] != -1 , \"Strength\"] = parents[\"pages.id\"].apply(lambda x: articleClassificationStrength(a, x))\n",
    "    parents.sort_values(by = [\"Strength\"], ascending = False, inplace = True)\n",
    "   \n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosenPathArticleToMTC(a):\n",
    "    strongestParent = strongestArticleParents(a)\n",
    "    path = chosenPathUpToMTC(strongestParent.iloc[0,1])\n",
    "    \n",
    "    path.loc[-1] = strongestParent.iloc[0, :3]\n",
    "    path.sort_index(inplace = True)\n",
    "    path.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run \"question to wikipedia category\" analyses on list of search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take list with search terms\n",
    "# Run getWikipediaInfo() until search term works\n",
    "# If result is False (search term not found) or \"Database call not successful (error)\" (search term found but path to MTC not available)\n",
    "    # --> continue to next search term\n",
    "# Return [categoriesFound (boolean), [n x Result], [1 x successful output]]\n",
    "\n",
    "#@stopit.threading_timeoutable(default='Find question categories jammed:' + str(maxSearchTime * 4) + ' seconds)')\n",
    "\n",
    "# Behaviour of stopit is erratic. Nesting stopits (here and in getCategoryInfo) leaads to unexpected results. Does however not seem to make loop fail if output formatted correctly.\n",
    "# --> Ensure jamming error is distinct, repeat processes for jammed indexes later.\n",
    "\n",
    "@stopit.threading_timeoutable(default='findQuestionCategories() jammed')\n",
    "def findQuestionCategories(a):\n",
    "    categoriesFound = False\n",
    "    result = []\n",
    "    getWikipediaInfo_out = [None, None, None, None]\n",
    "    toReturn = []\n",
    "    possibleFailureMessages = ('Search term not found', 'Database call timed out (' + str(maxSearchTime) + ' seconds)', 'Database call not successful (error)')\n",
    "    \n",
    "    for term in a:\n",
    "        termResult = getWikipediaInfo(term)        \n",
    "        \n",
    "        if termResult not in possibleFailureMessages:\n",
    "            categoriesFound = True\n",
    "            # CHANGE: NOT NECESSARY TO SAVE TERM\n",
    "            result.append( (\"SUCCESS\") )\n",
    "            getWikipediaInfo_out = termResult\n",
    "            break\n",
    "        else:\n",
    "            result.append( (termResult) )\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        result.append( (\"NO SEARCH TERMS GIVEN\") )\n",
    "    \n",
    "    # Insert categorieFound  at start of toReturn\n",
    "    toReturn.append(categoriesFound)\n",
    "    toReturn.append(result)\n",
    "    toReturn.append(getWikipediaInfo_out)\n",
    "    \n",
    "    return toReturn\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run \"question to wikipedia category\" analyses in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run code on given data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems that running batches > 1 leads to jamming not caught by maxSearchTime for stopit in getCategoryInfo\n",
    "# This is probably related to .apply not working together with neo4j database calls\n",
    "# --> Easiest to resolv by running only batches of n = 1 (does not seem to give significant)\n",
    "batchSize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfToIterate = t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchRuns = pd.read_csv(\"../workproduct-files/batchRuns.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndex = batchRuns.iloc[-1,1] + 1\n",
    "stopIndex = startIndex + batchSize - 1\n",
    "batchNr = len(batchRuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-0b2f5f3b6b43>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  workingDF[\"mostSimilar\"] = \"False\"\n",
      "<ipython-input-27-0b2f5f3b6b43>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  workingDF[\"mostSimilar\"] = \"False\"\n",
      "<ipython-input-27-0b2f5f3b6b43>:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  workingDF[\"mostSimilar\"] = \"False\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP!\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "while startIndex <= len(dfToIterate) - 1:\n",
    "    \n",
    "    if (len(dfToIterate) - startIndex) < batchSize:\n",
    "        stopIndex = len(dfToIterate) - 1\n",
    "    \n",
    "    dfToProcess = pd.DataFrame(dfToIterate.loc[startIndex : stopIndex , \"searchTerms\"])\n",
    "    \n",
    "    startTime = time.gmtime()\n",
    "    \n",
    "    # Run findQuestionCategories() and clean result\n",
    "    #dfToProcess[\"findQuestionCategories_Out\"] = dfToProcess[\"searchTerms\"].apply(lambda x: findQuestionCategories(x, timeout = 1))\n",
    "    dfToProcess[\"findQuestionCategories_Out\"] = dfToProcess[\"searchTerms\"].apply(lambda x: findQuestionCategories(x))\n",
    "    \n",
    "    # ADD:\n",
    "    # For loop that iterates over rows in dfToProcess\n",
    "    # If values is \"findQuestionCategories() jammed\"\n",
    "        # Then run findQuestionCategories again with timeout = MaxSearchTime\n",
    "    # If value is still \"Find question categories jammed\"\n",
    "        # Then change result to correctly formated list\n",
    "            # [categoriesFound (boolean), [n x Result], [1 x successful output]]\n",
    "            # --> [False, [\"findQuestionCategories() jammed\"], [None, None, None, None]]\n",
    "        \n",
    "    for row in dfToProcess.index:\n",
    "        if dfToProcess.loc[row, \"findQuestionCategories_Out\"] == 'findQuestionCategories() jammed':\n",
    "            #secondTry = findQuestionCategories(dfToProcess.loc[row, \"searchTerms\"], timeout = 10)\n",
    "            secondTry = findQuestionCategories(dfToProcess.loc[row, \"searchTerms\"])\n",
    "            \n",
    "            if secondTry == 'findQuestionCategories() jammed':\n",
    "                dfToProcess.loc[row, \"findQuestionCategories_Out\"] = [False, [\"findQuestionCategories() jammed\"], [None, None, None, None]]\n",
    "                \n",
    "            else:\n",
    "                dfToProcess.loc[row, \"findQuestionCategories_Out\"] = secondTry\n",
    "       \n",
    "    \n",
    "    \n",
    "    dfToProcess[['wikipediaSearchSuccessful','findQuestionCategories_meta', 'findQuestionCategories_result']] = pd.DataFrame(dfToProcess[\"findQuestionCategories_Out\"].tolist(), index= dfToProcess.index)\n",
    "    dfToProcess.drop(columns = [\"findQuestionCategories_Out\"], inplace = True)\n",
    "    dfToProcess[[\"wikipediaArticleTitle\", \"wikipediaArticleID\", \"categoryPath\", \"parentCategories\"]] = pd.DataFrame(dfToProcess[\"findQuestionCategories_result\"].tolist(), index= dfToProcess.index)\n",
    "    dfToProcess.drop(columns = [\"findQuestionCategories_result\"], inplace = True)\n",
    "    \n",
    "    # Save result to pickle in dedicated folder\n",
    "    dfToProcess.to_pickle(\"../workproduct-files/batchRuns/batch\" + str(batchNr) + \"_\" + str(startIndex) + \"-\" + str(stopIndex) +  \".pkl\")\n",
    "    \n",
    "    # Update metadata to csv\n",
    "    endTime = time.gmtime()\n",
    "    runTime = time.mktime(endTime)-time.mktime(startTime)\n",
    "    startTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", startTime)\n",
    "    endTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", endTime)\n",
    "    newRow = [startIndex, stopIndex, startTime, endTime, runTime]\n",
    "    batchRuns = batchRuns.append(pd.Series(newRow, index = batchRuns.columns), ignore_index = True)\n",
    "    batchRuns.to_csv(\"../workproduct-files/batchRuns.csv\", sep=\";\", index = False)\n",
    "    \n",
    "    # Read new parameters from csv for next loop\n",
    "    batchRuns = pd.read_csv(\"../workproduct-files/batchRuns.csv\", delimiter=\";\")\n",
    "    startIndex = batchRuns.iloc[-1,1] + 1\n",
    "    stopIndex = startIndex + batchSize - 1\n",
    "    batchNr = len(batchRuns)\n",
    "    \n",
    "    if (len(dfToIterate) - startIndex) < batchSize:\n",
    "        stopIndex = len(dfToIterate) - 1\n",
    "    \n",
    "    print(\"LOOP!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ALL DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
